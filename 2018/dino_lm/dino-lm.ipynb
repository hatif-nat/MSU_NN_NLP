{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Все должно исправно работать с версией tensorflow 1.8.0. Если что-то не так, проверяйте версию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "7371d8d1-959b-4362-af19-8986f44365a2"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import reduce\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "093d59b9-800c-4d67-9a1c-d60405b91dec"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c6564d20-7eb9-4e02-923b-e973df48ba81"
    }
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf5fa732-5247-4e41-82d3-db1bcc3787ac"
    }
   },
   "source": [
    "Из файла с именами динозавров считываем все, разбиваем все имена на символы, и конкатенируем. Получаем большой массив символов на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "4953d9b9-f668-45c2-86bd-1fcc57bd9216"
    }
   },
   "outputs": [],
   "source": [
    "def _read_chars(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        names = f.read().lower().split()\n",
    "        split_names = list(map(lambda x: list(x), names))\n",
    "        chars = reduce(lambda x, y: x + y, split_names)\n",
    "        return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "07a3418d-c927-4c0f-b893-fa37f07af799"
    }
   },
   "source": [
    "Считываем имена в один список строк. Затем каждое имя разбиваем на символы и перед ними вставляем токен начала последовательности - \"<start\\>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "0ea8aa82-1404-4597-ab99-b19870a4fb13"
    }
   },
   "outputs": [],
   "source": [
    "def _read_names(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        names = f.read().lower().split()\n",
    "        split_names = list(map(lambda x: ['<start>'] + list(x), names))\n",
    "        return split_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b03ce23c-e66f-4069-abd6-4bd59549a99e"
    }
   },
   "source": [
    "По списку символов строим словарь char_to_id, с помощью которого можно по символу получить его индекс, и словарь id_to_char, с помощью которого можно по индексу получить символ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "c5c0f080-502f-43f0-91ee-761617850c0c"
    }
   },
   "outputs": [],
   "source": [
    "def _build_vocab(filename):\n",
    "    data = _read_chars(filename)\n",
    "    \n",
    "    special_tokens = ['<start>', '<eos>', '<pad>']\n",
    "    \n",
    "    data += special_tokens\n",
    "    \n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    chars, _ = list(zip(*count_pairs))\n",
    "    char_to_id = dict(zip(chars, range(len(chars))))\n",
    "    id_to_char = {v: k for k, v in char_to_id.items()}\n",
    "\n",
    "    return char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d44ebc89-bee7-444b-9d3a-a4788c872869"
    }
   },
   "source": [
    "Функция _names_to_char_ids - то что получилось на выходе _read_names, переводим в индексы.\n",
    "```\n",
    "Пример:\n",
    "Выход функции _read_names.\n",
    "['<start>', 'm', 'e', 'l', 'a', 'n', 'o', 'r', 'o', 's', 'a', 'u', 'r', 'u', 's']\n",
    "['<start>', 'l', 'u', 's', 'i', 't', 'a', 'n', 'o', 's', 'a', 'u', 'r', 'u', 's']\n",
    "['<start>', 'j', 'i', 'a', 'n', 'g', 'j', 'u', 'n', 'm', 'i', 'a', 'o', 's', 'a', 'u', 'r', 'u', 's']\n",
    "['<start>', 'a', 's', 't', 'r', 'o', 'd', 'o', 'n']\n",
    "['<start>', 'o', 'u', 'r', 'a', 'n', 'o', 's', 'a', 'u', 'r', 'u', 's']\n",
    "\n",
    "Выход функции _names_to_char_ids.\n",
    "[[28, 14, 7, 9, 0, 5, 4, 3, 4, 1, 0, 2, 3, 2, 1],\n",
    " [28, 9, 2, 1, 6, 8, 0, 5, 4, 1, 0, 2, 3, 2, 1],\n",
    " [28, 22, 6, 0, 5, 13, 22, 2, 5, 14, 6, 0, 4, 1, 0, 2, 3, 2, 1],\n",
    " [28, 0, 1, 8, 3, 4, 15, 4, 5],\n",
    " [28, 4, 2, 3, 0, 5, 4, 1, 0, 2, 3, 2, 1]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "0a5b3f84-0ac0-4c16-86e9-99bb5eeb4696"
    }
   },
   "outputs": [],
   "source": [
    "def _names_to_char_ids(filename, char_to_id):\n",
    "    data = _read_names(filename)\n",
    "    \n",
    "    def name_processing(name):\n",
    "        return [char_to_id[char] for char in name]\n",
    "    \n",
    "    return list(map(name_processing, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5f0bb2f4-274d-4b6f-b56f-de9e62a47da6"
    }
   },
   "source": [
    "Функция ptb_raw_data возвращает данные для обучения, подбора параметров и итоговой оценки модели, а также словари char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbpresent": {
     "id": "dde6730b-37c5-423d-9cf2-8303661423ef"
    }
   },
   "outputs": [],
   "source": [
    "def ptb_raw_data(data_path=None):\n",
    "\n",
    "    train_path = os.path.join(data_path, \"dino.train.txt\")\n",
    "    dev_path = os.path.join(data_path, \"dino.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"dino.test.txt\")\n",
    "\n",
    "    char_to_id, id_to_char = _build_vocab(train_path)\n",
    "    train_data = _names_to_char_ids(train_path, char_to_id)\n",
    "    dev_data = _names_to_char_ids(dev_path, char_to_id)\n",
    "    test_data = _names_to_char_ids(test_path, char_to_id)\n",
    "    vocabulary = len(char_to_id)\n",
    "    print('Vocab len', vocabulary)\n",
    "    return train_data, dev_data, test_data, char_to_id, id_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1f866ba2-c908-4356-8d60-78adacf120a8"
    }
   },
   "source": [
    "Функция _batch_generator разбивает данные на батчи. Возращает вход и выход языковой модели для каждого батча. Выходы это те же входы только смещенные на 1, и с токеном < eos > в конце. \n",
    "```\n",
    "Пример:\n",
    "X = [[<start>, 1, 2, 3],\n",
    "     [<start>, 1, 2, 3, 4],\n",
    "     [<start>, 1, 2]]\n",
    "Y = [[1, 2, 3, <eos>],\n",
    "     [1, 2, 3, 4, <eos>],\n",
    "     [1, 2, <eos>]]\n",
    "```\n",
    "\n",
    "Далее каждая строка дополняется токеном < pad > до длины максимального имени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbpresent": {
     "id": "b5870b04-4653-4c77-b74e-e4267ced4671"
    }
   },
   "outputs": [],
   "source": [
    "def _batch_generator(data, batch_size, char_to_id, num_steps):\n",
    "    n = len(data)\n",
    "    X = data\n",
    "    Y = list(map(lambda x: x[1:] + [char_to_id['<eos>']], X))\n",
    "\n",
    "    batch_padded_X = sequence.pad_sequences(X, \n",
    "                                            maxlen=num_steps,\n",
    "                                            padding='post', \n",
    "                                            truncating='post', \n",
    "                                            value=char_to_id['<pad>'])\n",
    "\n",
    "    batch_padded_Y = sequence.pad_sequences(Y, \n",
    "                                            maxlen=num_steps,\n",
    "                                            padding='post', \n",
    "                                            truncating='post', \n",
    "                                            value=char_to_id['<pad>'])\n",
    "    \n",
    "    for k in range(0, n - n % batch_size, batch_size):\n",
    "        x = np.asarray(batch_padded_X[k: min(k+batch_size, n)])\n",
    "        y = np.asarray(batch_padded_Y[k: min(k+batch_size, n)])\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0c6ebfec-eb4e-47c9-8430-3eeaf8c216ad"
    }
   },
   "source": [
    "Класс ModelInput нужен для того, чтобы каждая из моделей (для train, dev и test) имела свой объект для работы с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXTFXg9luQNn",
    "nbpresent": {
     "id": "a2273a53-2136-4a44-98bf-7f8825acec61"
    }
   },
   "outputs": [],
   "source": [
    "class ModelInput(object):\n",
    "    def __init__(self, config, data, char_to_id, id_to_char, name=None):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        self.char_to_id = char_to_id\n",
    "        self.id_to_char = id_to_char\n",
    "        self.num_steps = config['num_steps']\n",
    "        self.batch_size = batch_size = config['batch_size']\n",
    "        self.epoch_size = len(data) // batch_size\n",
    "        \n",
    "    def batch_generator(self):\n",
    "        return _batch_generator(self.data, \n",
    "                               self.batch_size, \n",
    "                               self.char_to_id,\n",
    "                               self.num_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWoShmEfuQN7",
    "nbpresent": {
     "id": "5fe2751d-76ea-4e6e-b26e-4896b383c5d1"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c0b3d0db-c209-44f9-948f-61d2f8d3e287"
    }
   },
   "source": [
    "## Создание плейсхолдеров\n",
    "```\n",
    "Создадим ячейки для наших входных данных:\n",
    "1) `lstm_inputs` - ячейка для входных индексов символов размера (batch_size, num_steps)\n",
    "2) `lstm_targets` - ячейка в которую помещаются символы из `lstm_inputs` сдвинутые на один влево (то есть предсказания слова на следующем таймстепе)\n",
    "3) `lr` - ячейка для значения learning rate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgeU1IWFuQN-",
    "nbpresent": {
     "id": "092124b9-324e-4f72-8f1d-ed62df033a5d"
    }
   },
   "outputs": [],
   "source": [
    "def build_placeholders(network, config):\n",
    "\n",
    "    network['lstm_inputs'] = tf.placeholder(shape=(config['batch_size'], \n",
    "                                                   config['num_steps']), \n",
    "                                            dtype=tf.int32,\n",
    "                                            name=\"lstm_inputs\")\n",
    "\n",
    "    network['lstm_targets'] = tf.placeholder(shape=(config['batch_size'], \n",
    "                                                    config['num_steps']), \n",
    "                                             dtype=tf.int32, \n",
    "                                             name=\"lstm_targets\")\n",
    "\n",
    "    network['lr'] = tf.placeholder(shape=(), dtype=tf.float32, name=\"lr\")\n",
    "    \n",
    "    non_pad_ids = tf.not_equal(network['lstm_inputs'], config['pad_id'])\n",
    "    network['seq_lens'] = tf.reduce_sum(tf.cast(non_pad_ids, tf.int32), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5234000e-2678-4904-9e52-e1978862c4ea"
    }
   },
   "source": [
    "## Создание слоя векторных представлений слов  \n",
    "Слой, преобразующий индексы символов в векторные представления этих символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8nVMuY9JuQOG",
    "nbpresent": {
     "id": "206fc960-f50f-46b6-8f4d-70b4c2ae1a28"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_layer(network, config, is_training):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", \n",
    "                                    shape=[config['vocab_size'], \n",
    "                                           config['emb_size']], \n",
    "                                    dtype=tf.float32)\n",
    "        network['inputs'] = tf.nn.embedding_lookup(embedding, network['lstm_inputs'])\n",
    "\n",
    "    if is_training and config['keep_prob'] < 1:\n",
    "        network['inputs'] = tf.nn.dropout(network['inputs'], \n",
    "                                          config['keep_prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e99fb3e-4f8a-4270-9363-fb8e4bf711a3"
    }
   },
   "source": [
    "## Создание LSTM ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbpresent": {
     "id": "366fa989-ad6d-4c00-a565-98da64cf4cd9"
    }
   },
   "outputs": [],
   "source": [
    "def make_cell(config, is_training):\n",
    "    cell = tf.contrib.rnn.LSTMBlockCell(config['hidden_size'], \n",
    "                                        forget_bias=0.0)\n",
    "    if is_training and config['keep_prob'] < 1:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, \n",
    "                                             output_keep_prob=config['keep_prob'])\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8e2bbd79-5128-4271-a1ce-bc1893618f5a"
    }
   },
   "source": [
    "## Развёртывание LSTM последовательности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d536357c-65e6-4c18-989e-9543b7540c6c"
    }
   },
   "source": [
    "На вход функции dynamic_rnn подается нулевое начальное состояние, LSTM ячейка, входы рекуррентного слоя, и длина каждой последовательности в батче, чтобы не обрабатывать паддинги. В результате получаются выходы с каждого таймстепа, и финальные состояния h и c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5sO2-I2uQON",
    "nbpresent": {
     "id": "62b974af-03ea-42ac-89f3-922d3be147db"
    }
   },
   "outputs": [],
   "source": [
    "def build_rnn_graph(network, config, is_training):\n",
    "    network['cell'] = make_cell(config, is_training)\n",
    "\n",
    "    initial_state = network['cell'].zero_state(config['batch_size'], \n",
    "                                               dtype=tf.float32)\n",
    "    network['initial_state'] = initial_state\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(inputs=network['inputs'], \n",
    "                                             cell=network['cell'], \n",
    "                                             initial_state=network['initial_state'],\n",
    "                                             sequence_length=network['seq_lens'])\n",
    "\n",
    "    network['rnn_outputs'] = outputs\n",
    "    network['rnn_final_state'] = final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9e6d153f-2f14-4096-b0aa-1e8d09459f5c"
    }
   },
   "source": [
    "## Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf20f98a-8d34-4620-bd49-7c7bb37f79bc"
    }
   },
   "source": [
    "Для вычисления функции потерь требуется маска - матрица размером (batch_size, num_steps), в каждой ячейке которой стоит 0 или 1, обозначающий присутствие или отсутствие паддинга в этой позиции. network['softmax'] здесь вычисляется для того, чтобы генерировать имена, после обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GynGmOpOuQPU",
    "nbpresent": {
     "id": "15db254c-1e2b-4da3-9f17-1e62f40abf3c"
    }
   },
   "outputs": [],
   "source": [
    "def build_loss_function(network, config):\n",
    "    logits = tf.contrib.layers.fully_connected(network['rnn_outputs'], \n",
    "                                               num_outputs=config['vocab_size'],\n",
    "                                               activation_fn=None)\n",
    "\n",
    "    network['softmax'] = tf.nn.softmax(logits, axis=2)\n",
    "\n",
    "    sequence_mask = tf.sequence_mask(network['seq_lens'], \n",
    "                                     maxlen=config['num_steps'], \n",
    "                                     dtype=tf.float32)\n",
    "    \n",
    "    losses = tf.contrib.seq2seq.sequence_loss(logits,\n",
    "                                              network['lstm_targets'],\n",
    "                                              weights=sequence_mask,\n",
    "                                              average_across_timesteps=False,\n",
    "                                              average_across_batch=False)\n",
    "\n",
    "    total_words = tf.cast(tf.reduce_sum(network['seq_lens']), tf.float32)\n",
    "\n",
    "    network['loss'] = tf.reduce_sum(losses) / total_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d60709ba-620c-4f90-8157-4e7f18b864a6"
    }
   },
   "source": [
    "## Создание функции обучения\n",
    "Также в этой функции происходит обрезание градиента по значению 'max_grad_norm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWtPq3cRuQPX",
    "nbpresent": {
     "id": "bf2362a5-fafc-416d-ab3e-f011e7e9d7af"
    }
   },
   "outputs": [],
   "source": [
    "def build_train_ops(network, config):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(network['loss'], tvars), \n",
    "                                      config['max_grad_norm'])\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=network['lr'])\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate=network['lr'])\n",
    "\n",
    "    network['train_op'] = optimizer.apply_gradients(\n",
    "        zip(grads, tvars), \n",
    "        global_step=tf.train.get_or_create_global_step())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e0620461-9149-412a-bb4f-85635f805d25"
    }
   },
   "source": [
    "## Создание графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whaTk5HWuQPb",
    "nbpresent": {
     "id": "a51ca97b-0064-462f-a59f-ecefab8e1cbd"
    }
   },
   "outputs": [],
   "source": [
    "def build_graph(config, input_, is_training):\n",
    "    network = {'input': input_}\n",
    "\n",
    "    build_placeholders(network, config)\n",
    "\n",
    "    build_embedding_layer(network, config, is_training)\n",
    "\n",
    "    build_rnn_graph(network, config, is_training)\n",
    "\n",
    "    build_loss_function(network, config)\n",
    "\n",
    "    if is_training:\n",
    "        build_train_ops(network, config)\n",
    "\n",
    "    print(\"Trainable variables:\")\n",
    "    for var in tf.trainable_variables():\n",
    "        print(var.name, ':',  var.shape)\n",
    "    print()\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c573b96-3538-4d22-94b9-79198adf2173"
    }
   },
   "source": [
    "## Функция для прохождения одной эпохи обучения\n",
    "Здесь, с помощью функции batch_generator итерируемся по каждому батчу. Модель принимает входы, выходы, learning rate и нулевое начальное состояние. Функция run_epoch возвращает перплексию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFGhVSdluQPe",
    "nbpresent": {
     "id": "6574e2bb-078f-4c43-9b1f-b9c609fe3929"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, lr, config, model, eval_op=None, verbose=False):\n",
    "    start_time = time.time()\n",
    "    loss = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    fetches = {\n",
    "            \"loss\": model['loss'],\n",
    "            \"rnn_final_state\": model['rnn_final_state']\n",
    "    }\n",
    "\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "    def make_zero_state():\n",
    "        state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "            c = np.zeros((config['batch_size'], config['hidden_size'])), \n",
    "            h = np.zeros((config['batch_size'], config['hidden_size']))\n",
    "        )\n",
    "\n",
    "        return state\n",
    "\n",
    "    initial_state = make_zero_state()\n",
    "    for step, (X, Y) in enumerate(model['input'].batch_generator()):\n",
    "\n",
    "        feed_dict = {\n",
    "                     model['lstm_inputs']: X, \n",
    "                     model['lstm_targets']: Y, \n",
    "                     model['initial_state']: initial_state,\n",
    "                     model['lr']: lr\n",
    "                    }\n",
    "\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        local_loss = vals[\"loss\"]\n",
    "#         initial_state = vals[\"rnn_final_state\"]\n",
    "\n",
    "        loss += local_loss\n",
    "        iters += 1\n",
    "\n",
    "    return np.exp(loss / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "640aa790-c0ad-4725-9c9c-9e92393e8da5"
    }
   },
   "source": [
    "## Функция для сэмплирования имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nbpresent": {
     "id": "504d2f67-e492-4204-bd22-bedc514bf0b8"
    }
   },
   "outputs": [],
   "source": [
    "def sample(session, config, model):\n",
    "    fetches = {\n",
    "            \"rnn_final_state\": model['rnn_final_state'],\n",
    "            \"softmax\": model['softmax']\n",
    "    }\n",
    "\n",
    "    \n",
    "    def make_zero_state():\n",
    "        state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "            c = np.zeros((config['batch_size'], config['hidden_size'])), \n",
    "            h = np.zeros((config['batch_size'], config['hidden_size']))\n",
    "        )\n",
    "\n",
    "        return state\n",
    "    \n",
    "    initial_state = make_zero_state()\n",
    "    \n",
    "    start_id = model['input'].char_to_id['<start>']\n",
    "    \n",
    "    idx = np.ones((config['batch_size'], config['num_steps']), dtype=np.int32) * start_id\n",
    "    \n",
    "    result = []\n",
    "    for i in range(config['num_steps']):\n",
    "        fetches['softmax'] = model['softmax']\n",
    "        feed_dict = {\n",
    "                     model['lstm_inputs']: np.array(idx),\n",
    "                     model['initial_state']: initial_state,\n",
    "                    }\n",
    "\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        initial_state = vals[\"rnn_final_state\"]\n",
    "        \n",
    "        next_words = np.argmax(vals['softmax'], axis=2)[:, i]\n",
    "#         if i == 0:\n",
    "#             next_words[:] = model['input'].char_to_id['s']\n",
    "        \n",
    "        if i < config['num_steps'] - 1:\n",
    "            idx[:, i+1] = next_words\n",
    "\n",
    "        result.append([model['input'].id_to_char[x] for x in next_words])\n",
    "\n",
    "    result = list(zip(*result))\n",
    "    result = [''.join(r) for r in result][0]\n",
    "    result = result.split('<eos>')[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "Rr36kSAxuQPi",
    "nbpresent": {
     "id": "5c210b10-01a4-4860-b78a-e2dcf5116a69"
    },
    "outputId": "7f404d34-52e5-4b51-9d4b-bb8e87e5ca80",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len 29\n",
      "Trainable variables:\n",
      "Model/embedding:0 : (30, 200)\n",
      "Model/rnn/lstm_cell/kernel:0 : (400, 800)\n",
      "Model/rnn/lstm_cell/bias:0 : (800,)\n",
      "Model/fully_connected/weights:0 : (200, 30)\n",
      "Model/fully_connected/biases:0 : (30,)\n",
      "\n",
      "Trainable variables:\n",
      "Model/embedding:0 : (30, 200)\n",
      "Model/rnn/lstm_cell/kernel:0 : (400, 800)\n",
      "Model/rnn/lstm_cell/bias:0 : (800,)\n",
      "Model/fully_connected/weights:0 : (200, 30)\n",
      "Model/fully_connected/biases:0 : (30,)\n",
      "\n",
      "Trainable variables:\n",
      "Model/embedding:0 : (30, 200)\n",
      "Model/rnn/lstm_cell/kernel:0 : (400, 800)\n",
      "Model/rnn/lstm_cell/bias:0 : (800,)\n",
      "Model/fully_connected/weights:0 : (200, 30)\n",
      "Model/fully_connected/biases:0 : (30,)\n",
      "\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "Epoch: 1 Train Perplexity: 16.792\n",
      "Epoch: 1 Dev Perplexity: 12.910\n",
      "aus\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "Epoch: 2 Train Perplexity: 11.330\n",
      "Epoch: 2 Dev Perplexity: 9.190\n",
      "aus\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "Epoch: 3 Train Perplexity: 8.767\n",
      "Epoch: 3 Dev Perplexity: 7.851\n",
      "aurus\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "Epoch: 4 Train Perplexity: 7.714\n",
      "Epoch: 4 Dev Perplexity: 7.286\n",
      "aurus\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "Epoch: 5 Train Perplexity: 7.158\n",
      "Epoch: 5 Dev Perplexity: 7.033\n",
      "aurus\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "Epoch: 6 Train Perplexity: 6.941\n",
      "Epoch: 6 Dev Perplexity: 6.909\n",
      "aurus\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "Epoch: 7 Train Perplexity: 6.838\n",
      "Epoch: 7 Dev Perplexity: 6.858\n",
      "aurus\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "Epoch: 8 Train Perplexity: 6.788\n",
      "Epoch: 8 Dev Perplexity: 6.834\n",
      "aurus\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "Epoch: 9 Train Perplexity: 6.762\n",
      "Epoch: 9 Dev Perplexity: 6.822\n",
      "aurus\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "Epoch: 10 Train Perplexity: 6.750\n",
      "Epoch: 10 Dev Perplexity: 6.816\n",
      "aurus\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "Epoch: 11 Train Perplexity: 6.743\n",
      "Epoch: 11 Dev Perplexity: 6.813\n",
      "aurus\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "Epoch: 12 Train Perplexity: 6.740\n",
      "Epoch: 12 Dev Perplexity: 6.812\n",
      "aurus\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "Epoch: 13 Train Perplexity: 6.738\n",
      "Epoch: 13 Dev Perplexity: 6.811\n",
      "aurus\n",
      "Test Perplexity: 6.938\n"
     ]
    }
   ],
   "source": [
    "def get_small_config():\n",
    "    config = {'lr': 1.0, 'lr_decay': 0.5,\n",
    "              'max_grad_norm': 5, 'emb_size': 200,\n",
    "              'hidden_size': 200, 'keep_prob': 1.0, \n",
    "              'max_epoch': 4, 'max_max_epoch': 13, \n",
    "              'batch_size': 20, 'vocab_size': 30,\n",
    "              'rnn_type': 'dynamic_rnn'}\n",
    "    return config\n",
    "\n",
    "def get_small_eval_config():\n",
    "    eval_config = get_small_config()\n",
    "#     eval_config['batch_size'] = 1\n",
    "#     eval_config['num_steps'] = 1\n",
    "    return eval_config\n",
    "\n",
    "def main():\n",
    "\n",
    "    config = get_small_config()\n",
    "\n",
    "    data_path = 'DINO/'\n",
    "\n",
    "#     save_path = None\n",
    "#     restore_path = 'models/epoch_13'\n",
    "    save_path = 'models/'\n",
    "    restore_path = None\n",
    "    \n",
    "    decayed_lr = config['lr']\n",
    "\n",
    "    if  save_path is not None and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    raw_data = ptb_raw_data(data_path)\n",
    "    train_data, dev_data, test_data, char_to_id, id_to_char = raw_data\n",
    "\n",
    "    max_len = max(map(len, train_data + dev_data + test_data))\n",
    "    config['num_steps'] = max_len + 1\n",
    "\n",
    "    config['pad_id'] = char_to_id['<pad>']\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "        \n",
    "        with tf.name_scope(\"Train\"):\n",
    "            train_input = ModelInput(config=config, \n",
    "                                   data=train_data, \n",
    "                                   char_to_id=char_to_id, \n",
    "                                   id_to_char=id_to_char, \n",
    "                                   name=\"TrainInput\")\n",
    "\n",
    "            with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "                train_model = build_graph(config, input_=train_input, is_training=True)\n",
    "\n",
    "        with tf.name_scope(\"Dev\"):\n",
    "            dev_input = ModelInput(config=config, \n",
    "                                 data=dev_data,\n",
    "                                 char_to_id=char_to_id, \n",
    "                                 id_to_char=id_to_char, \n",
    "                                 name=\"DevInput\")\n",
    "\n",
    "            with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "                dev_model = build_graph(config, input_=dev_input, is_training=False)\n",
    "\n",
    "        with tf.name_scope(\"Test\"):\n",
    "            test_input = ModelInput(config=config, \n",
    "                                  data=test_data,\n",
    "                                  char_to_id=char_to_id, \n",
    "                                  id_to_char=id_to_char,\n",
    "                                  name=\"TestInput\")\n",
    "\n",
    "            with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "                test_model = build_graph(config, input_=test_input, is_training=False)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Если хотим загрузить обученную модель, то restore_path должен быть не None, пример сверху\n",
    "            if restore_path is not None:\n",
    "                saver = tf.train.import_meta_graph(restore_path + '.meta')\n",
    "                saver.restore(sess, tf.train.latest_checkpoint('models/'))\n",
    "            \n",
    "            # Цикл до максимального числа эпох, после каждой эпохи обучения, замеряем перплексию на dev\n",
    "            for i in range(config['max_max_epoch']):\n",
    "                lr_decay = config['lr_decay'] ** max(i + 1 - config['max_epoch'], 0.0)\n",
    "                decayed_lr = config['lr'] * lr_decay\n",
    "\n",
    "                print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, decayed_lr))\n",
    "                train_perplexity = run_epoch(sess, \n",
    "                                             decayed_lr, \n",
    "                                             config, \n",
    "                                             train_model, \n",
    "                                             eval_op=train_model['train_op'], \n",
    "                                             verbose=True)\n",
    "                print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "\n",
    "                dev_perplexity = run_epoch(sess, decayed_lr, config, dev_model)\n",
    "                print(\"Epoch: %d Dev Perplexity: %.3f\" % (i + 1, dev_perplexity))\n",
    "\n",
    "                name = sample(sess, config, test_model)\n",
    "                print(name)\n",
    "\n",
    "            # Итоговая оценка обученной модели\n",
    "            test_perplexity = run_epoch(sess, \n",
    "                                        decayed_lr, \n",
    "                                        config, \n",
    "                                        test_model)\n",
    "    \n",
    "            print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "            \n",
    "            # Сохраняем обученную модель\n",
    "            if save_path is not None:\n",
    "                filename = os.path.join(save_path, 'epoch_{}'.format(config['max_max_epoch']))\n",
    "                saver.save(sess, filename)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "98642214-9dc6-4389-999b-069f6677dc73"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f39490c4-d42e-4a06-b60c-00320139994b"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b6414ffa-7022-4a93-9041-584c126b56c6"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6f7adec6-898b-43d6-be90-c38d94b1ca48"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "49550b5a-bf35-4424-94e8-5d8545257640"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "87150313-421c-4e56-b3b3-ce3f72ea6a3f"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
